{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f49183-e2b8-4e56-aa55-7294d2d9baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print('PyTorch Version: {}'.format(torch.__version__))\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available: {}'.format(torch.cuda.get_device_name(0)))\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    print('GPU unavailable')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792e9c2-23e6-4ef5-acf6-6e8aa55e3bda",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f9f9e-9b62-4ac4-85d8-2fb147b5ea8c",
   "metadata": {},
   "source": [
    "First load an existing dictionary matrix (for inference or fine-tune) or initialize a new one (for training from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46a8d2-a1c3-4842-97e5-39b8b9096506",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_atoms = 3800\n",
    "feature_dim = 1600\n",
    "\n",
    "# Load existing dictionary\n",
    "path_to_dictionary = \"\"\n",
    "D = torch.load(path_to_dictionary,map_location=device) # The shape should be (n_atoms, feature_dim)\n",
    "\n",
    "# Or initialize a new dictionary\n",
    "D = torch.randn(n_atoms, feature_dim)/np.sqrt(feature_dim)\n",
    "D = D/torch.linalg.vector_norm(D,axis=1,keepdims=True)\n",
    "torch.save(D,path_to_dictionary) # Save the dictionary if you need to use the trained model later\n",
    "D = D.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f13fe4-fb36-4e5c-bca7-3e6540dda5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize autoencoder components\n",
    "from CAE_components import Encoder, Decoder\n",
    "encoder = Encoder(out_dim=n_atoms).to(device)\n",
    "decoder = Decoder(out_dim=feature_dim).to(device)\n",
    "\n",
    "# Load model weights if you have them\n",
    "# encoder.load_state_dict\n",
    "# decoder.load_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682dc10-6ca0-4a1f-8e3b-99cdfd3613b7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c903410-83af-4595-adaf-ab8263ce2be0",
   "metadata": {},
   "source": [
    "Prepare your own training and testing datasets and dataloader. Note that image size should be 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd3bca-8d67-4c75-a49c-0f89ae4086c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam([{'params': encoder.parameters(),'lr': 1e-4},\n",
    "                              {'params': decoder.parameters(),'lr': 1e-4}])\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442c8dd-e55d-4d96-9843-7108489eb448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose that you already have train_dataloader and batch_size\n",
    "\n",
    "# Set weights for loss function, they can be sensitive and vary for different datasets\n",
    "lam1 = 1.0\n",
    "lam2 = 0.0072 \n",
    "lam3 = 0.00001\n",
    "\n",
    "total_epoch = 50\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "for epoch in range(1,total_epoch+1):\n",
    "    for i,sample in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        img = sample['img'].to(device)\n",
    "        prob_weights = encoder(img)\n",
    "        img_feature = prob_weights@D\n",
    "        img_reconstruct = decoder(img_feature)\n",
    "    \n",
    "        loss_reconstruction = F.mse_loss(img,img_reconstruct)\n",
    "        loss_entropy = -torch.sum(prob_weights*torch.log(prob_weights+1e-10))/batch_size # Add a small term to avoid nan\n",
    "        loss_dirichlet = -torch.sum(torch.log(torch.mean(prob_weights+1e-10,dim=0))) \n",
    "        loss = lam1*loss_reconstruction + lam2*loss_entropy + lam3*loss_dirichlet\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca1c02-6153-4b6c-ba1b-dc8040a3d2c3",
   "metadata": {},
   "source": [
    "# Testing (img reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d9a7c-0bd8-4f2f-9f08-f419ac5477a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have test images tensor: test_img\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "test_img = test_img.to(device)\n",
    "test_img_reconstruct = decoder(encoder(test_img)@D)\n",
    "\n",
    "# You can check the MSE loss and plot the reconstructed images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa51d66-ac92-4d2f-8de5-4cfdd23184d5",
   "metadata": {},
   "source": [
    "For downstream tasks, discard the decoder and add a classifer head (MLP) to the end of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b9ebe-1e82-4b9f-8510-9d2a29b58e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
